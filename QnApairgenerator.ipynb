!pip install fitz transformers nltk pandas
!pip install pymupdf
!pip install keybert


import fitz  # PyMuPDF for PDF text extraction
import re
import nltk
from transformers import pipeline
from keybert import KeyBERT

nltk.download('punkt')
from nltk.tokenize import sent_tokenize
nltk.download('punkt_tab')

#Load the models
question_generator = pipeline("text2text-generation", model="valhalla/t5-small-qg-hl")
qa_model = pipeline("question-answering", model="deepset/roberta-base-squad2")

# For topic tagging
kw_model = KeyBERT()

#Extract Text from a Specific Page Range
def extract_text_from_pdf(pdf_path, start_page, end_page):
    doc = fitz.open(pdf_path)
    text = ""
    total_pages = len(doc)

    start_page = max(0, start_page - 1)  #index the pages
    end_page = min(total_pages, end_page)

    for page_num in range(start_page, end_page):
        text += doc[page_num].get_text("text") + "\n"

    return text.strip()

#Clean and Preprocess Text (Remove Subheadings, Extra Spaces)
def preprocess_text(text):
    text = re.sub(r'\s+', ' ', text)
    sentences = sent_tokenize(text)

    filtered_sentences = []
    for sent in sentences:
        if len(sent.split()) > 4 and not sent.isupper():
            filtered_sentences.append(sent)

    return filtered_sentences

#Generate Questions
def generate_question(sentence):
    input_text = "generate question: " + sentence
    result = question_generator(input_text, max_length=50)
    return result[0]['generated_text']

#Extract Answers with Sliding Window Context
def extract_answer(question, sentences, window_size=2):
    best_answer = ""
    best_confidence = 0

    for i in range(len(sentences) - window_size):
        context = " ".join(sentences[i : i + window_size])
        try:
            result = qa_model(question=question, context=context)
            answer = result['answer']
            confidence = result.get('score', 0)

            if len(answer.split()) > 2 and not answer.isupper():
                if confidence > best_confidence:
                    best_answer = answer
                    best_confidence = confidence
        except:
            continue

    return best_answer if best_answer else "Answer not found."

#Tag Topic/Keywords using KeyBERT
def get_topic_keywords(text, top_n=2):
    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)
    return [kw[0] for kw in keywords]

#Main Function with Topic Tagging
#so this block basically executes the entire workflow. Extracts plain text from the pdf, preprocesses it.
def generate_qa_from_pdf(pdf_path, start_page, end_page):
    pdf_text = extract_text_from_pdf(pdf_path, start_page, end_page)
    sentences = preprocess_text(pdf_text)

#For first 15 sentences(tags) produces QA pairs using both t5 and roberta models.
    print(f"\n **QA Pairs from Pages {start_page} to {end_page}:**\n")
    for sentence in sentences[:15]:
        try:
            question = generate_question(sentence)
            answer = extract_answer(question, sentences)

            if answer and answer.lower() != question.lower():
                combined_text = f"{question} {answer}"
                topics = get_topic_keywords(combined_text)
                tag_string = ", ".join(topics)

                print(f"**Q:** {question}")
                print(f"**A:** {answer}")
                print(f"   **Topic Tags:** {tag_string}\n")
        except Exception as e:
            print(f"Error processing sentence: {sentence}\n{e}")

#Upload the pdf and provide the page range from which QnA pairs have to be produced
pdf_path = "/content/polymer.pdf"
start_page = 8
end_page = 12
generate_qa_from_pdf(pdf_path, start_page, end_page)
